# üîç Smart RAG API

A production-ready **Retrieval-Augmented Generation (RAG)** API built with FastAPI that can answer questions based on information extracted from any document type.

##  Features

- **Multi-format Document Support**: PDF, Word (.docx), Text, Images (OCR), CSV, SQLite
- **OCR Capabilities**: Extract text from scanned documents and images using Tesseract
- **Vector Search**: Efficient similarity search using FAISS
- **Multiple LLM Support**: OpenAI GPT, Anthropic Claude
- **Multimodal Queries**: Support for image-based questions
- **RESTful API**: Clean FastAPI endpoints with automatic documentation
- **Web UI**: Optional Streamlit frontend
- **Docker Ready**: Full containerization support


```

## Project Structure

```
rag-api/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ main.py              # FastAPI application & endpoints
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py # Document parsing & chunking
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py         # Embedding generation
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py       # FAISS vector database
‚îÇ   ‚îî‚îÄ‚îÄ llm_handler.py        # LLM integration
‚îú‚îÄ‚îÄ uploads/                   # Uploaded documents
‚îú‚îÄ‚îÄ vector_store/             # Persisted FAISS index
‚îú‚îÄ‚îÄ streamlit_app.py          # Web UI
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ .env.example
‚îî‚îÄ‚îÄ README.md
```

## Quick Start

### Option 1: Local Installation

```bash
# Clone the repository
git clone <repo-url>
cd rag-api

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install Tesseract OCR (system dependency)
# Ubuntu/Debian:
sudo apt-get install tesseract-ocr
# macOS:
brew install tesseract
# Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki

# Set up environment variables
cp .env.example .env
# Edit .env and add your API keys

# Run the server
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Option 2: Docker

```bash
# Build and run with Docker Compose
docker-compose up --build

# Or just the API
docker build -t rag-api .
docker run -p 8000:8000 -e OPENAI_API_KEY=your-key rag-api
```

## Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `OPENAI_API_KEY` | OpenAI API key for embeddings & LLM | Yes* |
| `ANTHROPIC_API_KEY` | Anthropic API key for Claude | Optional |
| `EMBEDDING_MODEL` | Model for embeddings (default: `all-MiniLM-L6-v2`) | No |
| `LLM_MODEL` | LLM model name (default: `gpt-3.5-turbo`) | No |

*Required if using OpenAI. The API can fall back to SentenceTransformers for embeddings.

## API Documentation

Once running, visit:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Endpoints

#### `POST /upload` - Upload a Document

Upload and index a document for querying.

```bash
curl -X POST "http://localhost:8000/upload" \
  -F "file=@document.pdf"
```

**Response:**
```json
{
  "file_id": "a1b2c3d4",
  "filename": "document.pdf",
  "file_type": ".pdf",
  "chunks_created": 15,
  "status": "success",
  "message": "Document processed successfully. 15 chunks indexed."
}
```

#### `POST /query` - Ask a Question

Query the indexed documents.

```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What are the payment terms mentioned in the invoice?",
    "top_k": 5
  }'
```

**With Image (OCR):**
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is written in this image?",
    "image_base64": "base64_encoded_image_data"
  }'
```

**Response:**
```json
{
  "answer": "Based on the document, the payment terms are Net 30 days...",
  "context": [
    {
      "text": "Payment terms: Net 30 days from invoice date...",
      "metadata": {
        "filename": "invoice.pdf",
        "page": 2,
        "score": 0.89
      }
    }
  ],
  "sources": [
    {
      "filename": "invoice.pdf",
      "page": "2",
      "chunk_index": "5",
      "relevance_score": "0.892"
    }
  ],
  "confidence": 0.89,
  "processing_time": 1.23
}
```

#### `GET /files` - List Indexed Files

```bash
curl "http://localhost:8000/files"
```

#### `DELETE /files/{file_id}` - Delete a File

```bash
curl -X DELETE "http://localhost:8000/files/a1b2c3d4"
```

#### `GET /stats` - System Statistics

```bash
curl "http://localhost:8000/stats"
```

## üñ•Ô∏è Web Interface (Streamlit)

Run the Streamlit UI:

```bash
streamlit run streamlit_app.py
```

Access at http://localhost:8501

##  How It Works

### 1. Document Ingestion
```
Document ‚Üí Parser ‚Üí Text Extraction ‚Üí Chunking (with overlap)
                         ‚Üì
              [Optional: OCR for images/scanned PDFs]
```

### 2. Embedding & Storage
```
Text Chunks ‚Üí Embedding Model ‚Üí Vectors ‚Üí FAISS Index
                                    ‚Üì
                         Metadata stored alongside
```

### 3. Query Processing
```
Question ‚Üí Embedding ‚Üí Vector Search ‚Üí Top-K Chunks
                                            ‚Üì
              Context + Question ‚Üí LLM ‚Üí Answer
```

### 4. Chunking Strategy

The system uses intelligent chunking:
- **Chunk Size**: 1000 characters (configurable)
- **Overlap**: 200 characters between chunks
- **Sentence Boundaries**: Chunks break at sentence boundaries when possible

This ensures context is preserved across chunk boundaries.

## Testing

```bash
# Run tests
pytest tests/

# Test the API manually
python -c "
import requests

# Upload a document
with open('sample.pdf', 'rb') as f:
    r = requests.post('http://localhost:8000/upload', files={'file': f})
    print(r.json())

# Query
r = requests.post('http://localhost:8000/query', json={
    'question': 'What is this document about?'
})
print(r.json())
"
```

## Evaluation Criteria Coverage

| Criteria | Implementation | Weight |
|----------|---------------|--------|
| File parsing & preprocessing | PyMuPDF, python-docx, pandas, pytesseract | 20% |
| Vector search + RAG flow |  FAISS, sentence-transformers, context retrieval | 20% |
| Image OCR handling | pytesseract, base64 image support | 15% |
| API design & FastAPI usage |  RESTful endpoints, Pydantic models, async | 15% |
| Prompt engineering & LLM | Context-aware prompts, OpenAI/Claude | 15% |
| Bonus features | Docker, Streamlit UI, multi-doc support | 15% |

## Security Considerations

- API keys stored in environment variables
- File uploads validated by extension
- Input sanitization for queries
- No persistent storage of sensitive data

## Production Deployment

For production:

1. Use a proper database (PostgreSQL) for file metadata
2. Enable HTTPS with a reverse proxy (nginx)
3. Set up proper authentication (OAuth2/API keys)
4. Use Redis for caching embeddings
5. Scale with Kubernetes or Docker Swarm
6. Monitor with Prometheus/Grafana

## License

MIT License

## Contributing

Pull requests welcome! Please follow the existing code style and add tests for new features.

---

Built with using FastAPI, FAISS, and OpenAI
